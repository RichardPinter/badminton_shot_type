# src/api.py
"""
Main API Module for the Badminton Player Grading System.

Provides the GradingAPI class, a high-level interface to interact with the
entire grading module. Orchestrates feature processing, model training,
prediction, explanation generation, and persistence, using the
'Achievable Feature List' structure.

Author: Sujit
Role: Senior Machine Learning Engineer
"""

import numpy as np
import pandas as pd
import joblib
import os
import sys
import logging
from typing import List, Dict, Optional, Any, Tuple, Union

# Import module components and custom exceptions
try:
    from .preprocessing import FeaturePreprocessor
    from .conative_framework import ConativeFramework
    from .model import GradingModel, GradeClassification
    from .explanation import ExplanationGenerator
    from .exceptions import (
        GradingSystemError, ConfigurationError, DataError,
        ModelError, PreprocessingError, PredictionError
    )
except ImportError: # Fallback for direct execution
    from preprocessing import FeaturePreprocessor
    from conative_framework import ConativeFramework
    from model import GradingModel, GradeClassification
    from explanation import ExplanationGenerator
    from exceptions import (
        GradingSystemError, ConfigurationError, DataError,
        ModelError, PreprocessingError, PredictionError
    )

from sklearn.exceptions import NotFittedError

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Define standard grade mapping
GRADE_MAP_TO_NUMERIC = {'D': 0, 'C': 1, 'B': 2, 'A': 3}
GRADE_MAP_FROM_NUMERIC = {v: k for k, v in GRADE_MAP_TO_NUMERIC.items()}

# --- Define the MASTER list of ALL expected FLATTENED feature names ---
# --- TODO: This list MUST be finalized based on coordination with Utsab/Kabir ---
# --- and the exact output of the flattening logic in _preprocess_input_features ---
# --- It must include ALL keys generated by that logic in a FIXED order. ---
# Example based on achievable_features_list_cctv structure (needs completion):
EXPECTED_FLATTENED_FEATURE_NAMES = sorted([
    # Spatial Angles (Example: L/R, mean/std/min/max)
    'spatial_angle_elbow_angle_L_mean', 'spatial_angle_elbow_angle_L_std', 'spatial_angle_elbow_angle_L_min', 'spatial_angle_elbow_angle_L_max',
    'spatial_angle_elbow_angle_R_mean', 'spatial_angle_elbow_angle_R_std', 'spatial_angle_elbow_angle_R_min', 'spatial_angle_elbow_angle_R_max',
    'spatial_angle_knee_angle_L_mean', 'spatial_angle_knee_angle_L_std', 'spatial_angle_knee_angle_L_min', 'spatial_angle_knee_angle_L_max',
    'spatial_angle_knee_angle_R_mean', 'spatial_angle_knee_angle_R_std', 'spatial_angle_knee_angle_R_min', 'spatial_angle_knee_angle_R_max',
    'spatial_angle_shoulder_angle_L_mean', 'spatial_angle_shoulder_angle_L_std',
    'spatial_angle_shoulder_angle_R_mean', 'spatial_angle_shoulder_angle_R_std',
    'spatial_angle_hip_angle_L_mean', 'spatial_angle_hip_angle_L_std',
    'spatial_angle_hip_angle_R_mean', 'spatial_angle_hip_angle_R_std',
    # Relative Distances (Example: L/R, mean/std)
    'spatial_dist_norm_elbow_to_wrist_L_mean', 'spatial_dist_norm_elbow_to_wrist_L_std',
    'spatial_dist_norm_elbow_to_wrist_R_mean', 'spatial_dist_norm_elbow_to_wrist_R_std',
    'spatial_dist_norm_shoulder_to_hip_L_mean', 'spatial_dist_norm_shoulder_to_hip_L_std',
    'spatial_dist_norm_shoulder_to_hip_R_mean', 'spatial_dist_norm_shoulder_to_hip_R_std',
    'spatial_dist_norm_knee_to_ankle_L_mean', 'spatial_dist_norm_knee_to_ankle_L_std',
    'spatial_dist_norm_knee_to_ankle_R_mean', 'spatial_dist_norm_knee_to_ankle_R_std',
    # Court Positioning
    'court_positioning_avg_position_x', 'court_positioning_avg_position_y',
    'court_positioning_position_std_x', 'court_positioning_position_std_y',
    # Court Zones (Example for 9 zones)
    'court_positioning_zone_occupancy_freq_1', 'court_positioning_zone_occupancy_freq_2', 'court_positioning_zone_occupancy_freq_3',
    'court_positioning_zone_occupancy_freq_4', 'court_positioning_zone_occupancy_freq_5', 'court_positioning_zone_occupancy_freq_6',
    'court_positioning_zone_occupancy_freq_7', 'court_positioning_zone_occupancy_freq_8', 'court_positioning_zone_occupancy_freq_9',
    # Joint Velocities (Example: L/R, mean/std/max)
    'temporal_vel_wrist_velocity_L_mean', 'temporal_vel_wrist_velocity_L_std', 'temporal_vel_wrist_velocity_L_max',
    'temporal_vel_wrist_velocity_R_mean', 'temporal_vel_wrist_velocity_R_std', 'temporal_vel_wrist_velocity_R_max',
    'temporal_vel_elbow_velocity_L_mean', 'temporal_vel_elbow_velocity_L_std',
    'temporal_vel_elbow_velocity_R_mean', 'temporal_vel_elbow_velocity_R_std',
    'temporal_vel_knee_velocity_L_mean', 'temporal_vel_knee_velocity_L_std', 'temporal_vel_knee_velocity_L_max',
    'temporal_vel_knee_velocity_R_mean', 'temporal_vel_knee_velocity_R_std', 'temporal_vel_knee_velocity_R_max',
    'temporal_vel_ankle_velocity_L_mean', 'temporal_vel_ankle_velocity_L_std', 'temporal_vel_ankle_velocity_L_max',
    'temporal_vel_ankle_velocity_R_mean', 'temporal_vel_ankle_velocity_R_std', 'temporal_vel_ankle_velocity_R_max',
    # Joint Accelerations (Example: L/R, max, mean_positive)
    'temporal_accel_wrist_acceleration_L_max', 'temporal_accel_wrist_acceleration_L_mean_positive',
    'temporal_accel_wrist_acceleration_R_max', 'temporal_accel_wrist_acceleration_R_mean_positive',
    'temporal_accel_ankle_acceleration_L_max', 'temporal_accel_ankle_acceleration_L_mean_positive',
    'temporal_accel_ankle_acceleration_R_max', 'temporal_accel_ankle_acceleration_R_mean_positive',
    # Body Movement
    'body_movement_avg_body_speed_rally', 'body_movement_max_body_speed_rally',
    'body_movement_avg_body_acceleration_rally', 'body_movement_movement_distance_total',
    'body_movement_movement_distance_per_rally_avg',
    # Timing and Intensity
    'timing_and_intensity_avg_rally_duration', 'timing_and_intensity_avg_shots_per_rally',
    'timing_and_intensity_avg_time_between_shots', 'timing_and_intensity_avg_rest_time_between_rallies',
    'timing_and_intensity_play_rest_speed_ratio',
    # Stroke Profile (Example types)
    'stroke_profile_stroke_dist_smash', 'stroke_profile_stroke_dist_clear', 'stroke_profile_stroke_dist_drop',
    'stroke_profile_stroke_dist_net', 'stroke_profile_stroke_dist_drive', 'stroke_profile_stroke_dist_lift',
    'stroke_profile_unknown_stroke_proportion',
    # Technical Consistency Proxy
    'technical_consistency_proxy_elbow_angle_variability', 'technical_consistency_proxy_knee_angle_variability',
    'technical_consistency_proxy_wrist_speed_variability',
    'technical_consistency_proxy_overall_consistency_score', # Added proxy score key
    # Tactical Proxies
    'tactical_proxies_shot_placement_zone_entropy', 'tactical_proxies_net_play_frequency',
    'tactical_proxies_baseline_play_frequency', 'tactical_proxies_offensive_stroke_ratio',
    'tactical_proxies_composite_awareness_score', # Added composite score key
    # Court Coverage Metrics
    'court_coverage_metrics_coverage_area_hull', 'court_coverage_metrics_coverage_efficiency_ratio',
    'court_coverage_metrics_avg_repositioning_speed',
])


class GradingAPI:
    """High-level API for the Badminton Player Grading System.

    Orchestrates the workflow involving FeaturePreprocessor, ConativeFramework,
    GradingModel, and ExplanationGenerator.

    Provides methods for training the system, predicting grades for new players,
    and saving/loading the trained state.

    Attributes:
        config_path (str): Path to the configuration file.
        preprocessor (FeaturePreprocessor): Instance for preprocessing features.
        conative_framework (ConativeFramework): Instance for conative stage analysis.
        grading_model (GradingModel): Instance for the ML grading model.
        explanation_generator (ExplanationGenerator): Instance for generating explanations.
        expected_feature_names (List[str]): Master list of all expected flattened
            feature names in a fixed order.
    """

    def __init__(self, model_persist_path: Optional[str] = None, config_path: str = 'src/config.json'):
        """Initializes the GradingAPI.

        Initializes all components (Preprocessor, ConativeFramework, GradingModel,
        ExplanationGenerator). If `model_persist_path` is provided and valid,
        it attempts to load the saved API state using `load_state`.

        Args:
            model_persist_path: Optional path to a saved API state file (.joblib).
            config_path: Path to the configuration file (used for ConativeFramework
                and potentially saved/loaded with state).

        Raises:
            ConfigurationError: If ConativeFramework fails to initialize due to config issues.
            GradingSystemError: For other unexpected initialization errors.
            (Exceptions from `load_state` if loading fails, e.g., FileNotFoundError,
             ModelError, ConfigurationError).
        """
        self.config_path = config_path
        try:
            self.preprocessor: FeaturePreprocessor = FeaturePreprocessor()
            self.conative_framework: ConativeFramework = ConativeFramework(config_path=self.config_path)
            self.grading_model: GradingModel = GradingModel()
            self.explanation_generator: ExplanationGenerator = ExplanationGenerator(self.conative_framework)
            # Use the globally defined master list
            self.expected_feature_names: List[str] = EXPECTED_FLATTENED_FEATURE_NAMES
            logger.info(f"GradingAPI initialized. Expecting {len(self.expected_feature_names)} features. Config path: {config_path}")

            if model_persist_path:
                logger.info(f"Attempting to load saved API state from: {model_persist_path}")
                try:
                    self.load_state(model_persist_path)
                except (FileNotFoundError, ConfigurationError, ModelError) as e:
                    logger.error(f"Failed to load API state from {model_persist_path}: {e}. API remains uninitialized.", exc_info=False) # Don't need full trace here
                    self.__init__(config_path=config_path) # Re-initialize to a clean state
                except Exception as e:
                    logger.error(f"Unexpected error loading API state from {model_persist_path}: {e}. API remains uninitialized.", exc_info=True)
                    self.__init__(config_path=config_path) # Re-initialize
            else:
                logger.info("Initializing new API state (no model loaded).")
        except ConfigurationError as e:
            logger.critical(f"API Initialization failed due to config error: {e}", exc_info=False)
            # If config fails, the API is unusable, re-raise or handle gracefully depending on requirements
            raise ConfigurationError(f"API could not initialize due to critical configuration error: {e}") from e
        except Exception as e:
            logger.critical(f"Unexpected error during API initialization: {e}", exc_info=True)
            # Handle other unexpected initialization errors
            raise GradingSystemError(f"Unexpected error during API initialization: {e}") from e

    # --- Feature Flattening Helpers ---

    def _flatten_spatial_features(self, spatial: Dict, flat_features: Dict):
        """Helper to flatten spatial features from the input dict.

        Extracts joint angles, relative distances, and court positioning stats
        from the 'spatial' sub-dictionary and adds them to `flat_features`
        using standardized keys defined in `EXPECTED_FLATTENED_FEATURE_NAMES`.

        Args:
            spatial: The 'spatial_features' sub-dictionary from the raw input.
            flat_features: The dictionary where flattened features are accumulated.
        """
        if not isinstance(spatial, dict): return
        # Angles
        j_ang = spatial.get('joint_angles', {})
        if isinstance(j_ang, dict):
            for joint, stats in j_ang.items():
                if isinstance(stats, dict):
                    for stat, val in stats.items():
                        key = f'spatial_angle_{joint}_{stat}'
                        if key in self.expected_feature_names: flat_features[key] = val
        # Relative Distances
        r_dist = spatial.get('relative_joint_distances', {})
        if isinstance(r_dist, dict):
             for joint, stats in r_dist.items():
                  if isinstance(stats, dict):
                       for stat, val in stats.items():
                            key = f'spatial_dist_{joint}_{stat}'
                            if key in self.expected_feature_names: flat_features[key] = val
        # Court Positioning
        c_pos = spatial.get('court_positioning', {})
        if isinstance(c_pos, dict):
             for k, v in c_pos.items():
                  if k == 'zone_occupancy_freq' and isinstance(v, dict):
                       for zone, freq in v.items():
                            key = f'court_positioning_{k}_{zone}'
                            if key in self.expected_feature_names: flat_features[key] = freq
                  else:
                       key = f'court_positioning_{k}'
                       if key in self.expected_feature_names: flat_features[key] = v

    def _flatten_temporal_features(self, temporal: Dict, flat_features: Dict):
        """Helper to flatten temporal features from the input dict.

        Extracts joint velocities, accelerations, body movement, and timing stats
        from the 'temporal' sub-dictionary and adds them to `flat_features`
        using standardized keys.

        Args:
            temporal: The 'temporal_features' sub-dictionary from the raw input.
            flat_features: The dictionary where flattened features are accumulated.
        """
        if not isinstance(temporal, dict): return
        # Velocities
        j_vel = temporal.get('joint_velocities', {})
        if isinstance(j_vel, dict):
             for joint, stats in j_vel.items():
                  if isinstance(stats, dict):
                       for stat, val in stats.items():
                            key = f'temporal_vel_{joint}_{stat}'
                            if key in self.expected_feature_names: flat_features[key] = val
        # Accelerations
        j_accel = temporal.get('joint_accelerations', {})
        if isinstance(j_accel, dict):
             for joint, stats in j_accel.items():
                  if isinstance(stats, dict):
                       for stat, val in stats.items():
                            key = f'temporal_accel_{joint}_{stat}'
                            if key in self.expected_feature_names: flat_features[key] = val
        # Body Movement
        b_mov = temporal.get('body_movement', {})
        if isinstance(b_mov, dict):
             for k, v in b_mov.items():
                  key = f'body_movement_{k}'
                  if key in self.expected_feature_names: flat_features[key] = v
        # Timing and Intensity
        t_int = temporal.get('timing_and_intensity', {})
        if isinstance(t_int, dict):
             for k, v in t_int.items():
                  key = f'timing_and_intensity_{k}'
                  if key in self.expected_feature_names: flat_features[key] = v

    def _flatten_performance_metrics(self, perf_metrics: Dict, features_dict: Dict, flat_features: Dict, calculated_metrics: Optional[Dict] = None):
        """Helper to flatten performance metrics (derived), optionally using pre-calculated values.

        Extracts stroke profile, technical consistency proxies, tactical proxies,
        and court coverage metrics from the 'performance_metrics' sub-dictionary.

        If `calculated_metrics` (from ConativeFramework) is provided, it uses the
        pre-calculated 'technical_consistency' and 'tactical_awareness' scores.
        Otherwise, it calculates them on the fly using ConativeFramework methods.
        These calculated scores are added to `flat_features` under specific keys
        (e.g., 'technical_consistency_proxy_overall_consistency_score').

        Args:
            perf_metrics: The 'performance_metrics' sub-dictionary from the raw input.
            features_dict: The full raw feature dictionary (needed for on-the-fly calculations).
            flat_features: The dictionary where flattened features are accumulated.
            calculated_metrics: Optional dictionary containing pre-calculated metrics
                from `ConativeFramework.determine_stage`.
        """
        if not isinstance(perf_metrics, dict): return

        # Use provided calculated metrics if available, otherwise calculate them
        tech_consistency_score = calculated_metrics.get('technical_consistency') if calculated_metrics else None
        if tech_consistency_score is None and hasattr(self, 'conative_framework'):
            tech_consistency_score = self.conative_framework._calculate_technical_consistency_score(features_dict)

        tactical_awareness_score = calculated_metrics.get('tactical_awareness') if calculated_metrics else None
        if tactical_awareness_score is None and hasattr(self, 'conative_framework'):
            tactical_awareness_score = self.conative_framework._calculate_tactical_awareness(features_dict)

        # Stroke Profile
        s_prof = perf_metrics.get('stroke_profile', {})
        if isinstance(s_prof, dict):
             for k, v in s_prof.items():
                  key = f'stroke_profile_{k}'
                  if key in self.expected_feature_names: flat_features[key] = v
        # Tech Consistency Proxy
        t_cons = perf_metrics.get('technical_consistency_proxy', {})
        if isinstance(t_cons, dict):
             # Add the overall score (either pre-calculated or calculated now)
             if tech_consistency_score is not None and 'technical_consistency_proxy_overall_consistency_score' in self.expected_feature_names:
                 flat_features['technical_consistency_proxy_overall_consistency_score'] = tech_consistency_score
             # Also add raw variability metrics if they exist and are expected
             for k, v in t_cons.items():
                  key = f'technical_consistency_proxy_{k}'
                  # Avoid overwriting the overall score if it was added
                  if key in self.expected_feature_names and key != 'technical_consistency_proxy_overall_consistency_score':
                       flat_features[key] = v
        # Tactical Proxies
        tac_p = perf_metrics.get('tactical_proxies', {})
        if isinstance(tac_p, dict):
             # Add the overall score (either pre-calculated or calculated now)
             if tactical_awareness_score is not None and 'tactical_proxies_composite_awareness_score' in self.expected_feature_names:
                 flat_features['tactical_proxies_composite_awareness_score'] = tactical_awareness_score
             # Also add raw tactical metrics if they exist and are expected
             for k, v in tac_p.items():
                  key = f'tactical_proxies_{k}'
                  # Avoid overwriting the overall score if it was added
                  if key in self.expected_feature_names and key != 'tactical_proxies_composite_awareness_score':
                       flat_features[key] = v
        # Court Coverage Metrics
        cc_met = perf_metrics.get('court_coverage_metrics', {})
        if isinstance(cc_met, dict):
             for k, v in cc_met.items():
                  key = f'court_coverage_metrics_{k}'
                  if key in self.expected_feature_names: flat_features[key] = v

    # --- End Flattening Helpers ---

    def _preprocess_input_features(self, features_dict: Dict[str, Any], calculated_conative_metrics: Optional[Dict] = None) -> Tuple[pd.Series, List[str]]:
        """Validates, flattens, and standardizes a single raw input feature dictionary.

        Uses the flattening helper methods (`_flatten_spatial_features`, etc.) to
        convert the nested input dictionary into a flat dictionary.
        Passes `calculated_conative_metrics` to the performance flattening helper
        to avoid redundant calculations.
        Creates a pandas Series from the flat dictionary, ensuring it matches the
        order and features defined in `self.expected_feature_names`, filling
        missing values with NaN.

        Args:
            features_dict: The raw nested feature dictionary for a single player.
            calculated_conative_metrics: Optional dictionary of pre-calculated metrics
                from the conative stage determination.

        Returns:
            A tuple containing:
                - pd.Series: The feature vector with standardized keys and NaN for missing.
                - List[str]: The list of expected feature names used for ordering.

        Raises:
            DataError: If input is not a dictionary, flattening fails, or creating
                the final Series fails.
            GradingSystemError: If internal state (`expected_feature_names`) is missing.
        """
        # Validation: Check if input is a dictionary
        if not isinstance(features_dict, dict):
            err_msg = "Input features must be a dictionary."
            logger.error(err_msg)
            raise DataError(err_msg)

        flat_features = {}
        try:
            # Use helper methods for flattening, passing calculated metrics to performance helper
            self._flatten_spatial_features(features_dict.get('spatial_features', {}), flat_features)
            self._flatten_temporal_features(features_dict.get('temporal_features', {}), flat_features)
            self._flatten_performance_metrics(features_dict.get('performance_metrics', {}), features_dict, flat_features, calculated_conative_metrics)

        except Exception as e:
            err_msg = f"Error during feature dictionary flattening: {e}"
            logger.error(err_msg, exc_info=True)
            raise DataError(err_msg) from e

        # Create Pandas Series using the MASTER list, filling missing with NaN
        try:
            feature_vector = pd.Series(flat_features, dtype=float)
            if not self.expected_feature_names:
                raise GradingSystemError("API Internal Error: Expected feature names not set.")
            feature_vector = feature_vector.reindex(self.expected_feature_names)
            missing_count = feature_vector.isna().sum()
            if missing_count > 0:
                 missing_keys = feature_vector[feature_vector.isna()].index.tolist()
                 logger.warning(f"Feature vector created with {missing_count} missing features (NaN): {missing_keys[:10]}...")

            return feature_vector, self.expected_feature_names
        except Exception as e:
             err_msg = f"Error creating Pandas Series or reindexing: {e}"
             logger.error(err_msg, exc_info=True)
             raise DataError(f"Failed to create standardized feature vector Series: {err_msg}") from e

    def _preprocess_feature_list_for_training(self, features_list: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[str], List[int]]:
        """Processes a list of raw feature dictionaries into a NumPy array for training/fitting.

        Iterates through the input list, calling `_preprocess_input_features` for each.
        Collects successfully processed vectors and stacks them into a NumPy array.
        Skips samples that cause a `DataError` during individual processing.

        Args:
            features_list: A list of raw feature dictionaries.

        Returns:
            A tuple containing:
                - np.ndarray: The processed feature matrix (samples x features).
                - List[str]: The list of expected feature names (columns).
                - List[int]: Indices of the original samples that were successfully processed.

        Raises:
            DataError: If no samples could be processed successfully.
            GradingSystemError: If stacking the processed vectors fails (internal error).
        """
        logger.info(f"Preprocessing list of {len(features_list)} feature dictionaries for training...")
        processed_vectors = []
        valid_indices = []

        if not features_list:
            logger.warning("Input features_list is empty.")
            return np.array([]), [], []
        if self.expected_feature_names is None:
            # This shouldn't happen if __init__ ran correctly
            raise GradingSystemError("API Internal Error: Expected feature names not defined during list processing.")

        for i, features_dict in enumerate(features_list):
            try:
                # Use the single-item preprocessing method
                vector_series, _ = self._preprocess_input_features(features_dict)
                # _preprocess_input_features now raises DataError on failure
                processed_vectors.append(vector_series.values)
                valid_indices.append(i)
            except DataError as e:
                logger.warning(f"Skipping sample {i} due to DataError: {e}")
            except Exception as e: # Catch unexpected errors during processing
                logger.warning(f"Skipping sample {i} due to unexpected error during preprocessing: {e}", exc_info=True)

        if not processed_vectors:
            err_msg = "No feature dictionaries processed successfully from the input list."
            logger.error(err_msg)
            raise DataError(err_msg)

        try:
            X = np.vstack(processed_vectors)
            logger.info(f"Successfully processed {X.shape[0]} samples into feature matrix shape {X.shape}.")
            return X, self.expected_feature_names, valid_indices
        except ValueError as e:
            err_msg = f"Error stacking processed vectors: {e}. Check vector lengths."
            logger.error(err_msg, exc_info=True)
            # This indicates an internal inconsistency if vectors have different lengths after _preprocess_input_features
            raise GradingSystemError(f"Internal Error: {err_msg}") from e

    # --- Training Helpers ---

    def _fit_preprocessor_for_training(self, X_original: np.ndarray, y: np.ndarray, feature_names_original: List[str], k_features: Union[int, str]) -> Tuple[np.ndarray, List[str]]:
        """Fits the preprocessor and transforms the training data.

        Calls `FeaturePreprocessor.fit` and `FeaturePreprocessor.transform`.

        Args:
            X_original: Original feature matrix before preprocessing.
            y: Training labels.
            feature_names_original: Names corresponding to columns in X_original.
            k_features: Feature selection parameter ('all' or int).

        Returns:
            A tuple containing:
                - np.ndarray: The processed feature matrix after fitting and transforming.
                - List[str]: The names of the selected features.

        Raises:
            ModelError: Wraps exceptions from the preprocessor's fit/transform methods.
        """
        logger.info(f"Fitting FeaturePreprocessor on {X_original.shape[0]} valid samples...")
        try:
            self.preprocessor.fit(X_original, y, feature_names=feature_names_original, k=k_features)
            X_processed = self.preprocessor.transform(X_original)
            selected_feature_names = self.preprocessor.get_selected_feature_names()
            logger.info(f"Preprocessor fit complete. Processed data shape: {X_processed.shape}")
            return X_processed, selected_feature_names
        except PreprocessingError as e:
            logger.error(f"Training failed during FeaturePreprocessor fit/transform: {e}", exc_info=False)
            raise ModelError(f"Feature preprocessing failed: {e}") from e
        except Exception as e: # Catch unexpected errors
            logger.error(f"Unexpected error during FeaturePreprocessor fit/transform: {e}", exc_info=True)
            raise ModelError(f"Unexpected feature preprocessing error: {e}") from e

    def _train_grading_model(self, X_processed: np.ndarray, y: np.ndarray, selected_feature_names: List[str], cv_folds: int) -> np.ndarray:
        """Trains the GradingModel.

        Calls `GradingModel.train`.

        Args:
            X_processed: The preprocessed feature matrix.
            y: Training labels.
            selected_feature_names: Names corresponding to columns in X_processed.
            cv_folds: Number of cross-validation folds.

        Returns:
            NumPy array of cross-validation scores.

        Raises:
            ModelError: Wraps exceptions from the model's train method.
        """
        logger.info(f"Training GradingModel on {X_processed.shape[0]} processed samples...")
        try:
            cv_scores = self.grading_model.train(X_processed, y, feature_names=selected_feature_names, cv_folds=cv_folds)
            logger.info("GradingModel training complete.")
            return cv_scores
        except ModelError as e:
            logger.error(f"Training failed during GradingModel training: {e}", exc_info=False)
            raise ModelError(f"Grading model training failed: {e}") from e
        except Exception as e: # Catch unexpected errors
            logger.error(f"Unexpected error during GradingModel training: {e}", exc_info=True)
            raise ModelError(f"Unexpected grading model training error: {e}") from e

    # --- End Training Helpers ---

    def train(self, features_list: List[Dict[str, Any]], grades: List[str], k_features: Union[int, str] = 'all', cv_folds: int = 5) -> Dict[str, Any]:
        """Trains the grading system (Preprocessor and Grading Model).

        Orchestrates the training process:
        1. Prepares data: Converts list of dicts to NumPy array using
           `_preprocess_feature_list_for_training`, maps grades to numeric.
        2. Fits preprocessor: Calls `_fit_preprocessor_for_training`.
        3. Trains model: Calls `_train_grading_model`.
        4. Compiles and returns summary results.

        Args:
            features_list: List of raw feature dictionaries for training.
            grades: List of corresponding grade labels ('A', 'B', 'C', 'D').
            k_features: Feature selection parameter for the preprocessor.
            cv_folds: Number of cross-validation folds for the model.

        Returns:
            A dictionary containing training results (status, counts, selected features,
            CV scores, etc.).

        Raises:
            DataError: If input lists mismatch, are empty, or contain invalid grades.
            ModelError: If data preparation, preprocessing, or model training fails.
        """
        logger.info(f"Starting training: {len(features_list)} samples, k={k_features}, cv={cv_folds}.")
        if len(features_list) != len(grades):
            raise DataError("Features list and grades list length mismatch.")
        if not features_list:
            raise DataError("Cannot train on empty feature list.")

        # 1. Preprocess features & get valid indices
        try:
            X_original, feature_names_original, valid_indices = self._preprocess_feature_list_for_training(features_list)
            if X_original.shape[0] == 0:
                raise DataError("No valid features processed for training after filtering.")
            # Filter grades to match valid features
            grades_filtered = [grades[i] for i in valid_indices]
            y = np.array([GRADE_MAP_TO_NUMERIC[g] for g in grades_filtered])
        except (DataError, GradingSystemError) as e: # Catch errors from preprocessing list
            logger.error(f"Training failed during data preparation: {e}", exc_info=False)
            raise ModelError(f"Data preparation failed: {e}") from e
        except KeyError as e: # Invalid grade label
            err_msg = f"Invalid grade label found: {e}. Use only 'A', 'B', 'C', 'D'."
            logger.error(err_msg)
            raise DataError(err_msg) from e
        except Exception as e: # Catch unexpected errors during prep
            logger.error(f"Unexpected error during data preparation: {e}", exc_info=True)
            raise ModelError(f"Unexpected data preparation error: {e}") from e

        # 2. Fit Preprocessor (using helper)
        X_processed, selected_feature_names = self._fit_preprocessor_for_training(
            X_original, y, feature_names_original, k_features
        )

        # 3. Train Model (using helper)
        cv_scores = self._train_grading_model(
            X_processed, y, selected_feature_names, cv_folds
        )

        # 4. Compile and return results
        results = {
            'status': 'Training successful', 'num_samples_processed': X_processed.shape[0],
            'num_original_features': self.preprocessor.original_feature_count,
            'num_features_selected': len(selected_feature_names) if selected_feature_names else X_processed.shape[1],
            'selected_feature_names': selected_feature_names,
            'cv_scores': cv_scores.tolist() if cv_scores.size > 0 else [],
            'mean_cv_accuracy': float(np.mean(cv_scores)) if cv_scores.size > 0 else None,
            'std_cv_accuracy': float(np.std(cv_scores)) if cv_scores.size > 0 else None,
        }
        logger.info(f"Training Results: {results}")
        return results

    # --- Prediction Helper ---

    def _apply_ml_prediction(self, X_original_single: np.ndarray, final_output_args: Dict):
        """Applies preprocessor transform and ML model prediction.

        Transforms the single sample using the fitted preprocessor.
        If the grading model is trained, predicts the grade and probabilities,
        updating the `final_output_args` dictionary.
        If the model is not trained, sets grade based on conative stage and default confidence.

        Args:
            X_original_single: The 2D NumPy array for the single sample (1 row).
            final_output_args: The dictionary holding prediction results, to be updated.

        Raises:
            (Exceptions from `preprocessor.transform` or `grading_model.predict`/`predict_proba`
             are expected to be caught by the caller `predict_grade`).
        """
        # logger.debug("Applying fitted preprocessor...")
        X_processed_single = self.preprocessor.transform(X_original_single)
        selected_names = self.preprocessor.get_selected_feature_names()
        if selected_names:
            processed_features_dict = dict(zip(selected_names, X_processed_single[0]))
            final_output_args['processed_features'] = processed_features_dict # Update final args
        else:
            logger.warning("No selected feature names available from preprocessor.")

        # logger.debug(f"Preprocessing complete. Processed shape: {X_processed_single.shape}")

        if not self.grading_model.is_trained:
            logger.warning("Grading model not trained. Prediction based solely on Conative Stage.")
            final_output_args['grade'] = self.conative_framework.map_to_grade(final_output_args['conative_stage'])
            final_output_args['confidence'] = 0.5 # Indicate lower confidence
            final_output_args['explanation'] = "Grading model not trained; grade based on Conative Framework stage only."
        else: # Model is trained
            # logger.debug("Predicting grade and probabilities using trained model...")
            grade_idx = self.grading_model.predict(X_processed_single)[0]
            probabilities = self.grading_model.predict_proba(X_processed_single)[0]
            final_output_args['grade'] = GRADE_MAP_FROM_NUMERIC.get(grade_idx, 'N/A') # Update final args
            if 0 <= grade_idx < len(probabilities):
                final_output_args['confidence'] = float(probabilities[grade_idx]) # Update final args
            else:
                logger.warning(f"Predicted index {grade_idx} out of bounds for probs {probabilities}. Setting confidence to 0.")
                final_output_args['confidence'] = 0.0
            final_output_args['feature_importance'] = self.grading_model.get_feature_importance() # Update final args
            # Explanation will be generated after this helper returns

    # --- End Prediction Helper ---

    def predict_grade(self, features_dict: Dict[str, Any]) -> GradeClassification:
        """Predicts the grade for a single player's features dictionary.

        Orchestrates the prediction workflow:
        1. Determines conative stage and calculates metrics (`determine_stage`).
        2. Preprocesses input features, potentially using calculated metrics
           (`_preprocess_input_features`).
        3. Applies preprocessing transform and ML model prediction (`_apply_ml_prediction`)
           if the components are fitted/trained.
        4. Generates the final explanation (`generate_explanation`).

        Catches internal errors (DataError, ModelError, ConfigurationError, etc.)
        and returns a GradeClassification object with grade 'N/A' and the error
        message in the explanation field.

        Args:
            features_dict: Dictionary containing the raw features for a single player.
                Example structure (simplified):
                ```python
                {
                    'player_id': 'Player_Test_001',
                    'spatial_features': {
                        'joint_angles': {'elbow_angle_R': {'mean': 120, ...}, ...},
                        'court_positioning': {'avg_position_x': 0.1, ...}
                    },
                    'temporal_features': {
                        'joint_velocities': {'wrist_velocity_R': {'max': 4.0, ...}, ...},
                        'body_movement': {'avg_body_speed_rally': 2.0, ...}
                    },
                    'performance_metrics': {
                        'stroke_profile': {'stroke_dist_smash': 0.2, ...},
                        'technical_consistency_proxy': {'elbow_angle_variability': 15.0, ...}
                        # ... other performance sub-dicts
                    }
                }
                ```

        Returns:
        Returns a GradeClassification object, including error details in the
        explanation field if prediction fails. Does not raise exceptions itself,
        but catches them internally.
        """
        logger.info("Starting prediction for single sample...")
        player_id = features_dict.get('player_id', 'UnknownPlayer')
        # Default result structure
        final_output_args = {
            'grade': 'N/A', 'confidence': 0.0, 'conative_stage': 0, 'stage_scores': {},
            'explanation': 'Prediction pending...', 'feature_importance': None,
            'processed_features': None, 'player_id': player_id
        }
        calculated_metrics = None # Initialize calculated metrics

        try:
            # --- Step 0: Basic Checks ---
            if self.conative_framework.thresholds is None:
                raise ConfigurationError("Conative framework thresholds not loaded. API not properly initialized.")
            if not self.expected_feature_names:
                raise GradingSystemError("API Internal Error: Expected feature names not set.")

            # --- Step 1: Determine Conative Stage & Get Metrics --- # Changed order slightly
            determined_conative_stage, calculated_metrics = self.conative_framework.determine_stage(features_dict)
            final_output_args['conative_stage'] = determined_conative_stage
            # Store the calculated metrics dict, stage_scores is no longer used/relevant
            final_output_args['stage_scores'] = calculated_metrics # Re-purpose stage_scores for now, or add new field?

            # --- Step 2: Preprocess Input Dictionary (using calculated metrics) ---
            feature_vector_series, _ = self._preprocess_input_features(features_dict, calculated_conative_metrics=calculated_metrics)
            if feature_vector_series.isna().any():
                 nan_features = feature_vector_series[feature_vector_series.isna()].index.tolist()
                 logger.warning(f"Input features contain NaNs before imputation: {nan_features}. Preprocessor will handle them.")
            X_original_single = feature_vector_series.values.reshape(1, -1)

            # --- Step 3 & 4: Preprocessing Transform and Prediction ---
            if not self.preprocessor.is_fitted:
                logger.warning("Preprocessor not fitted. Prediction based solely on Conative Stage.")
                final_output_args['grade'] = self.conative_framework.map_to_grade(determined_conative_stage)
                final_output_args['confidence'] = 0.5
                final_output_args['explanation'] = "Preprocessor not trained; grade based on Conative Framework stage only."
            else:
                # Use helper for the ML part
                self._apply_ml_prediction(X_original_single, final_output_args)

            # --- Step 5: Generate Explanation ---
            if final_output_args['explanation'] == 'Prediction pending...':
                # Pass the actual features_vector (Series) if available, not the processed dict
                features_vector_for_exp = pd.Series(final_output_args['processed_features']) if final_output_args.get('processed_features') else None
                final_output_args['explanation'] = self.explanation_generator.generate_explanation(
                    grade=final_output_args['grade'], confidence=final_output_args['confidence'],
                    conative_stage=final_output_args['conative_stage'], feature_importance=final_output_args['feature_importance'],
                    features_vector=features_vector_for_exp
                )

            logger.info(f"Prediction successful: Grade={final_output_args['grade']}, Confidence={final_output_args['confidence']:.2f}, Stage={final_output_args['conative_stage']}")
            return GradeClassification(**final_output_args)

        # --- Exception Handling: Catch specific errors and update the explanation ---
        except (DataError, PreprocessingError, ModelError, ConfigurationError, GradingSystemError, NotFittedError) as e:
            msg = f"Prediction failed due to {type(e).__name__}: {e}"
            logger.error(msg, exc_info=False)
            final_output_args['grade'] = 'N/A'
            final_output_args['confidence'] = 0.0
            final_output_args['explanation'] = msg
            return GradeClassification(**final_output_args)
        except Exception as e: # Catch any other unexpected errors
            msg = f"An unexpected error occurred during prediction: {e}"
            logger.error(msg, exc_info=True)
            final_output_args['grade'] = 'N/A'
            final_output_args['confidence'] = 0.0
            final_output_args['explanation'] = msg
            return GradeClassification(**final_output_args)


    def save_state(self, filepath: str):
        """Saves the current state of the API (preprocessor, model, config, features)."""
        logger.info(f"Saving GradingAPI state to {filepath}...")
        if not self.preprocessor.is_fitted:
            raise ModelError("Cannot save API state: Preprocessor is not fitted.")
        if not self.grading_model.is_trained:
            # Allow saving even if model isn't trained, but preprocessor is? Or raise error?
            # Let's raise for consistency, as usually you save a *trained* state.
            raise ModelError("Cannot save API state: GradingModel is not trained.")
        if self.expected_feature_names is None:
             # This should be caught by __init__
             raise GradingSystemError("API Internal Error: Expected feature names not defined.")

        # Only attempt to create directories if the filepath includes a directory part
        dir_name = os.path.dirname(filepath)
        if dir_name:
             try: os.makedirs(dir_name, exist_ok=True)
             except OSError as e: raise ModelError(f"Could not create directory for saving state: {e}") from e

        api_state = {
            'preprocessor': self.preprocessor, 'grading_model': self.grading_model,
            'expected_feature_names': self.expected_feature_names, 'config_path': str(self.config_path) if self.config_path else None,
            'conative_framework_thresholds': self.conative_framework.thresholds
        }
        try:
            joblib.dump(api_state, filepath, compress=3)
            logger.info(f"GradingAPI state saved successfully to {filepath}")
        except Exception as e:
            logger.error(f"Error saving API state: {e}", exc_info=True)
            raise ModelError(f"Failed to save API state to {filepath}") from e

    def load_state(self, filepath: str):
        """Loads the API state (preprocessor, model, config, features) from a file."""
        logger.info(f"Loading GradingAPI state from {filepath}...")
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"API state file not found: {filepath}")

        try:
            api_state = joblib.load(filepath)
            required_keys = ['preprocessor', 'grading_model', 'expected_feature_names', 'config_path']
            if not all(key in api_state for key in required_keys):
                missing = set(required_keys) - set(api_state.keys())
                raise ModelError(f"Invalid saved state format. Missing keys: {missing}")

            # --- Type Validation ---
            if not isinstance(api_state['preprocessor'], FeaturePreprocessor):
                raise ModelError("Loaded 'preprocessor' object is not a FeaturePreprocessor.")
            if not isinstance(api_state['grading_model'], GradingModel):
                 raise ModelError("Loaded 'grading_model' object is not a GradingModel.")
            if not isinstance(api_state['expected_feature_names'], list):
                 raise ModelError("Loaded 'expected_feature_names' is not a list.")
            if not isinstance(api_state['config_path'], (str, type(None))):
                 raise ModelError("Loaded 'config_path' is not a string or None.")

            # --- Assign Loaded Components ---
            self.preprocessor = api_state['preprocessor']
            self.grading_model = api_state['grading_model']
            self.expected_feature_names = api_state['expected_feature_names']
            loaded_config_path = api_state['config_path']

            # --- Sanity Checks & Re-initialization of Dependents ---
            if not self.preprocessor.is_fitted:
                logger.warning("Loaded preprocessor state indicates it was not fitted. Predictions may fail.")
                # Don't force is_fitted = True, let subsequent calls fail if necessary.
            if not self.grading_model.is_trained:
                logger.warning("Loaded grading model state indicates it was not trained. Predictions may use Conative only.")
                # Don't force is_trained = True.

            # Re-initialize dependent components using the loaded config path
            self.config_path = loaded_config_path
            logger.info(f"Re-initializing ConativeFramework and ExplanationGenerator with loaded config path: {self.config_path}")
            # This ConativeFramework init might raise ConfigurationError if the config is now invalid/missing
            self.conative_framework = ConativeFramework(config_path=self.config_path)
            self.explanation_generator = ExplanationGenerator(self.conative_framework)

            # Verify expected features match loaded preprocessor's *original* features
            if self.preprocessor.original_feature_names and self.expected_feature_names != self.preprocessor.original_feature_names:
                 logger.warning("Loaded expected_feature_names differ from names stored in loaded preprocessor. Using names from loaded state.")
                 # Trust the state file's expected_feature_names as the master list
                 # self.expected_feature_names = self.preprocessor.original_feature_names # Or trust preprocessor?

            logger.info(f"GradingAPI state loaded successfully from {filepath}")

        except ConfigurationError as e: # Catch error from ConativeFramework re-init
             logger.error(f"Configuration error after loading state (checking config '{self.config_path}'): {e}", exc_info=False)
             # Reset state? Or leave partially loaded? Resetting seems safer.
             # Attempt to re-initialize with the *loaded* config path, which might fail again.
             # Need a default config path if the loaded one is bad. Let's assume 'src/config.json' is default.
             default_config_path = 'src/config.json'
             try: self.__init__(config_path=default_config_path)
             except ConfigurationError: logger.critical(f"Failed to re-initialize with default config {default_config_path} after load failure.")
             raise ConfigurationError(f"Failed to load API state due to config error: {e}") from e
        except (ModelError, FileNotFoundError, ValueError) as e: # Catch specific known load errors
            logger.error(f"Error loading API state from {filepath}: {e}", exc_info=False)
            # Reset state using default config path
            default_config_path = 'src/config.json'
            try: self.__init__(config_path=default_config_path)
            except ConfigurationError: logger.critical(f"Failed to re-initialize with default config {default_config_path} after load failure.")
            # Re-raise as ModelError for consistency in load failures
            raise ModelError(f"Failed to load API state: {e}") from e
        except Exception as e: # Catch unexpected errors during loading
            logger.error(f"Unexpected error loading API state from {filepath}: {e}", exc_info=True)
            # Reset state using default config path
            default_config_path = 'src/config.json'
            try: self.__init__(config_path=default_config_path)
            except ConfigurationError: logger.critical(f"Failed to re-initialize with default config {default_config_path} after load failure.")
            raise GradingSystemError(f"Unexpected error loading API state: {e}") from e # Use base error type

